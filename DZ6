{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7phh5f3SUZOg"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Pool\n",
        "from typing import List, Dict, Tuple, Callable, Iterable\n",
        "from collections import Counter, defaultdict\n",
        "import regex as re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh-CHygfUnS5",
        "outputId": "779ae1c0-b1c9-4b03-cf4e-5d0ddb25c58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "train_df = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/dz6/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/dz6/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jzHyIS6QH4pR"
      },
      "outputs": [],
      "source": [
        "class TrieNode:\n",
        "    \n",
        "    def __init__(self, char: str) -> None:\n",
        "        self.char: str = char\n",
        "        self.is_end: bool = False\n",
        "        self.count: int = 0\n",
        "        self.children: Dict = {}\n",
        "\n",
        "\n",
        "class Trie:\n",
        "\n",
        "    def __init__(self, words: Iterable[str] = None):\n",
        "        self.root = TrieNode(\"\")\n",
        "        \n",
        "        if words:\n",
        "            for word in words:\n",
        "                self.insert(word)\n",
        "        \n",
        "    def insert(self, word: str) -> None:\n",
        "        \n",
        "        node: TrieNode = self.root\n",
        "        \n",
        "        for char in word:\n",
        "            if char in node.children:\n",
        "                node = node.children[char]\n",
        "            else:\n",
        "                new_node: TrieNode = TrieNode(char)\n",
        "                node.children[char] = new_node\n",
        "                node = new_node\n",
        "        node.is_end = True\n",
        "        node.count += 1\n",
        "        \n",
        "    def dfs(self, node, prefix):\n",
        "        \n",
        "        if node.is_end:\n",
        "            self.output.append((prefix + node.char, node.count))\n",
        "            \n",
        "        for child in node.children.values():\n",
        "            self.dfs(child, prefix + node.char)\n",
        "    \n",
        "    def query(self, prefix: str) -> List[Tuple[str, int]]:\n",
        "        \n",
        "        self.output = []\n",
        "        node = self.root\n",
        "        \n",
        "        for char in prefix:\n",
        "            if char in node.children:\n",
        "                node = node.children[char]\n",
        "            else:\n",
        "                return []\n",
        "            \n",
        "        self.dfs(node, prefix[:-1])\n",
        "        \n",
        "        return sorted(self.output, key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O_0rvBqrHu7L"
      },
      "outputs": [],
      "source": [
        "def norm_tok(text: str) -> List[str]:\n",
        "    text = text.lower().replace('ё', 'е')\n",
        "    text = re.sub('[^\\p{L}0-9,.\\-?!–«»\"\": ]', ' ', text)\n",
        "    text = re.sub(' +', ' ', text).strip()\n",
        "    text = nltk.wordpunct_tokenize(text)\n",
        "    return text\n",
        "tok_text = train_df.sentence.apply(norm_tok).values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pvMnDT4HHxte"
      },
      "outputs": [],
      "source": [
        "ngrams_conf = {\"pad_left\": True, \"pad_right\": True, \"left_pad_symbol\": '<BOS>', \"right_pad_symbol\": '<EOS>'}\n",
        "def build_ngram_counts(tok_text: Iterable[Iterable[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
        "    counts = defaultdict(Counter)\n",
        "    for tokens in tok_text:\n",
        "        for ngram in nltk.ngrams(tokens, n=n, **ngrams_conf):\n",
        "            prefix, next_token = ngram[:-1], ngram[-1]\n",
        "            counts[' '.join(prefix)][next_token] += 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dfhgA__OH62R"
      },
      "outputs": [],
      "source": [
        "trie = Trie(word for sentence in tok_text for word in sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqBs0yC4H8nM",
        "outputId": "a92b8572-520b-4f85-c3d0-d937e86f731b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('нейронные', 7),\n",
              " ('нейронную', 5),\n",
              " ('нейронні', 4),\n",
              " ('нейронный', 3),\n",
              " ('нейронных', 3),\n",
              " ('нейронной', 3),\n",
              " ('нейронного', 3),\n",
              " ('нейронна', 3),\n",
              " ('нейронная', 3),\n",
              " ('нейронное', 2),\n",
              " ('нейронну', 2),\n",
              " ('нейронному', 1),\n",
              " ('нейронний', 1),\n",
              " ('нейронних', 1),\n",
              " ('нейронній', 1),\n",
              " ('нейронне', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "trie.query('нейронн')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0lnrt6tdOwkI"
      },
      "outputs": [],
      "source": [
        "class lModel:\n",
        "    def __init__(self, tok_text: Iterable[Iterable[str]], n: int) -> None:\n",
        "        self.n: int = n\n",
        "        self.probs: Dict[str, Dict[str, float]] = defaultdict(Counter)\n",
        "        for prefix, distribution in build_ngram_counts(tok_text, n=n).items():\n",
        "            norm = sum(distribution.values())\n",
        "            self.probs[prefix] = Counter({token: count / norm for token, count in distribution.items()})\n",
        "    def getN(self):\n",
        "        return self.n\n",
        "\n",
        "\n",
        "def trie_predict(sentence: str, trie: Trie) -> str:\n",
        "    to_predict = sentence.split()[-1].lower()\n",
        "    n, probs = lModel(tok_text, n=3)\n",
        "    before_word = (' '.join(sentence.split()[-mod.getN():-1])).lower()\n",
        "    predictions_lmodel = mod.probs[before_word]\n",
        "    if predictions_lmodel:\n",
        "        max_prob = -100\n",
        "        return_word = \"\"\n",
        "        for word, prob in predictions_lmodel.items():\n",
        "            if word.startswith(to_predict) and max_prob < prob:\n",
        "                max_prob = prob\n",
        "                return_word = word\n",
        "        if max_prob != -100:\n",
        "            return return_word\n",
        "    predictions_trie = trie.query(to_predict)\n",
        "    if predictions_trie:\n",
        "        return predictions_trie[0][0]\n",
        "    else:\n",
        "        return to_predict\n",
        "\n",
        "\n",
        "def pd_func(df) -> pd.DataFrame:\n",
        "    df['token'] = df['prefix'].apply(lambda x: trie_predict(x, trie))\n",
        "    return df\n",
        "\n",
        "\n",
        "def parallelize_dataframe(df: pd.DataFrame, func: Callable, n_cores: int) -> pd.DataFrame:\n",
        "    with Pool(n_cores) as pool:\n",
        "        results = pool.map(func, np.array_split(df, n_cores))\n",
        "    return pd.concat(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jwyjQrFOzAU"
      },
      "outputs": [],
      "source": [
        "mod = lModel(tok_text, n=3)\n",
        "prediction_df = parallelize_dataframe(test_df, pd_func, n_cores=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOvh8PhyPDTy"
      },
      "outputs": [],
      "source": [
        "prediction_df[['index', 'token']].to_csv(\"/content/gdrive/MyDrive/Colab Notebooks/dz6/baseline.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}